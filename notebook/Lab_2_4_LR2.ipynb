{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYMATJ8kleZD"
   },
   "source": [
    "# Laboratory 2.4: Linear Regression contd.\n",
    "\n",
    "In this practice you will extend your Linear Regression code to a more general case. Here you will need the `synthetic_dataset.csv` present in the .zip file you downloaded alongside this notebook.\n",
    "\n",
    "In addition, we will be using the following libraries:\n",
    "- Data management:\n",
    "    - [numpy](https://numpy.org/)\n",
    "    - [pandas](https://pandas.pydata.org/)\n",
    "    - [scipy](https://scipy.org/)\n",
    "- Modelling:\n",
    "    - [scikit-learn](https://scikit-learn.org)\n",
    "- Plotting:\n",
    "    - [seaborn](https://seaborn.pydata.org/)\n",
    "    - [matplotlib](https://matplotlib.org/)\n",
    "    \n",
    "### **All the things you need to do are marked by a \"TODO\" comment nearby. Make sure you *read carefully everything before working* and solve each point before submitting your solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xgMlbAFVleZE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVG3fnFnleZF"
   },
   "source": [
    "### Custom Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to be working with the following class that you already know from the previous practice, so feel free to recycle as much code as you want (or can). In this case, you will be enhancing its functionalities, getting a more general function than the one you implemented before. \n",
    "\n",
    "**For now, just continue with the practice and do not fill anything, you will come back later to fill the gaps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-CsK6Eq7leZF"
   },
   "outputs": [],
   "source": [
    "from src.Lab_2_4_LR2 import LinearRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**: Remember, from the previous lab session, we had the following univariate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data to use in this practice\n",
    "x = np.array([0, 3, 2, 1, 4, 6, 7, 8, 9, 10])\n",
    "y = np.array([2, 3, 2, 4, 5, 7, 9, 9, 10, 13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code *you wrote* from the previous practice (not the sklearn version) to fit the data in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegressor()\n",
    "linreg.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the things you'll do today, let's construct a function that provides the values for the $R^2$, RMSE and MAE. For this, the inputs of this function are the true $y$ values and the predicted $\\hat{y}$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Lab_2_4_LR2 import evaluate_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R2': array([-1.4279241 , -0.82044747, -1.4279241 , -0.36873407, -0.07278391,\n",
      "        0.05182668, -0.44661569, -0.44661569, -0.92948173, -3.3126593 ]), 'RMSE': array([5.58341701, 4.83472289, 5.58341701, 4.19220055, 3.71140748,\n",
      "       3.48920413, 4.30981965, 4.30981965, 4.97740349, 7.44140749]), 'MAE': 4.043636363636364}\n"
     ]
    }
   ],
   "source": [
    "# Obtain regression performance metrics\n",
    "y_pred = linreg.predict(x)\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous practice, you have trained a model assuming that the relationship between input and output is linear. However, in most real cases this is not common and the relationship between input and output is not linear. In this section, we are going to learn how to deal with non-linear relationships when using linear models. Read `synthetic_dataset.csv` and train a linear regression model. \n",
    "\n",
    "*The target variable is the last column of the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/synthetic_dataset.csv\")\n",
    "\n",
    "# TODO: Obtain inputs and output from data\n",
    "X = data[[\"Input1\", \"Input2\", \"Input3\", \"Input4\"]].values\n",
    "y = data[\"Output\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we will skip the preprocessing and go straight to the modelling phase. Therefore, fit the model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 35.02451098544287\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train linear regression model\n",
    "linreg = LinearRegressor()\n",
    "linreg.fit(X, y, \"gradient_descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R2': 0.5452814220208011,\n",
       " 'RMSE': 3.1936452824642383,\n",
       " 'MAE': 2.2373494491887844}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Obtain and print the regression performance metrics\n",
    "evaluate_regression(y, linreg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained the model well, the $R^2$ metric will be terrible, in the order of $0.5$. **What happened here?** Let's try to clear this via the *study of the residuals*.\n",
    "\n",
    "Create a function to plot the residuals of the model. This function shall:\n",
    "- Create a **histogram** of the residuals.\n",
    "- Create a **Q-Q plot** of the residuals.\n",
    "- Create a **scatterplot of the residuals against each input variable, the true output variable and the predictions**.\n",
    "\n",
    "**Why do we want to check the residuals this way?** \n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(data, output_column, prediction_column):\n",
    "    \"\"\"\n",
    "    Plot residuals of a model against all variables in the DataFrame, using box plots for\n",
    "    categorical variables and scatter plots for continuous variables. Additionally, plot\n",
    "    a histogram and a QQ-plot of the residuals.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        output_column (str): The name of the output column.\n",
    "        prediction_column (str): The name of the prediction column.\n",
    "    \"\"\"\n",
    "    # TODO: Calculate residuals\n",
    "    residuals = None \n",
    "    \n",
    "    num_features = len(data.columns) - 2  # Exclude output and prediction columns\n",
    "\n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_rows = int(np.ceil(np.sqrt(num_features + 4)))  # Add 4 for histogram, Q-Q plot, true output vs residuals, and predictions vs residuals\n",
    "    num_cols = int(np.ceil((num_features + 4) / num_rows))\n",
    "\n",
    "    # Plot histogram of residuals\n",
    "    plt.figure(figsize=(5 * num_cols, 4 * num_rows))\n",
    "    plt.subplot(num_rows, num_cols, 1)\n",
    "    plt.hist(residuals, bins=30, edgecolor='black')\n",
    "    plt.title('Histogram of Residuals')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # TODO: Plot Q-Q plot of residuals (tip: use stats.probplot from scipy)\n",
    "    plt.subplot(num_rows, num_cols, 2)\n",
    "    stats.probplot(None, None, None)       # Fill the code here\n",
    "    plt.title('Q-Q Plot of Residuals')\n",
    "\n",
    "    # TODO: Plot residuals against output variable\n",
    "    plt.subplot(num_rows, num_cols, 3)\n",
    "    plt.scatter(None, None, alpha=0.5)     # Fill the code here\n",
    "    plt.title('Residuals vs True Output')\n",
    "    plt.xlabel('True Output')\n",
    "    plt.ylabel('Residuals')\n",
    "\n",
    "    # Plot residuals against prediction variable\n",
    "    plt.subplot(num_rows, num_cols, 4)\n",
    "    plt.scatter(data[prediction_column], residuals, alpha=0.5)   # Use this as example for later\n",
    "    plt.title('Residuals vs Predictions')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Residuals')\n",
    "\n",
    "    # TODO: Plot residuals against each input variable\n",
    "    for i, col in enumerate(data.columns):\n",
    "        if col not in [output_column, prediction_column]:\n",
    "            plt.subplot(num_rows, num_cols, i + 5)\n",
    "            plt.scatter(None, None, alpha=0.5)   # Fill the code here\n",
    "            plt.title(f'Residuals vs {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **print the coefficients** of the model **alongside the plots** you can generate with the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of the model: None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plot the residuals for the predictions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linreg\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m----> 7\u001b[0m plot_residuals(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 24\u001b[0m, in \u001b[0;36mplot_residuals\u001b[1;34m(data, output_column, prediction_column)\u001b[0m\n\u001b[0;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m num_cols, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m num_rows))\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(num_rows, num_cols, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(residuals, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistogram of Residuals\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResiduals\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Javie\\Downloads\\Python\\Lib\\site-packages\\matplotlib\\pyplot.py:3221\u001b[0m, in \u001b[0;36mhist\u001b[1;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[0;32m   3196\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mhist)\n\u001b[0;32m   3197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhist\u001b[39m(\n\u001b[0;32m   3198\u001b[0m     x: ArrayLike \u001b[38;5;241m|\u001b[39m Sequence[ArrayLike],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3219\u001b[0m     BarContainer \u001b[38;5;241m|\u001b[39m Polygon \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[BarContainer \u001b[38;5;241m|\u001b[39m Polygon],\n\u001b[0;32m   3220\u001b[0m ]:\n\u001b[1;32m-> 3221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mhist(\n\u001b[0;32m   3222\u001b[0m         x,\n\u001b[0;32m   3223\u001b[0m         bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[0;32m   3224\u001b[0m         \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m,\n\u001b[0;32m   3225\u001b[0m         density\u001b[38;5;241m=\u001b[39mdensity,\n\u001b[0;32m   3226\u001b[0m         weights\u001b[38;5;241m=\u001b[39mweights,\n\u001b[0;32m   3227\u001b[0m         cumulative\u001b[38;5;241m=\u001b[39mcumulative,\n\u001b[0;32m   3228\u001b[0m         bottom\u001b[38;5;241m=\u001b[39mbottom,\n\u001b[0;32m   3229\u001b[0m         histtype\u001b[38;5;241m=\u001b[39mhisttype,\n\u001b[0;32m   3230\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m   3231\u001b[0m         orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   3232\u001b[0m         rwidth\u001b[38;5;241m=\u001b[39mrwidth,\n\u001b[0;32m   3233\u001b[0m         log\u001b[38;5;241m=\u001b[39mlog,\n\u001b[0;32m   3234\u001b[0m         color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m   3235\u001b[0m         label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[0;32m   3236\u001b[0m         stacked\u001b[38;5;241m=\u001b[39mstacked,\n\u001b[0;32m   3237\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3238\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3239\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Javie\\Downloads\\Python\\Lib\\site-packages\\matplotlib\\__init__.py:1478\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1480\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1482\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Javie\\Downloads\\Python\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:6772\u001b[0m, in \u001b[0;36mAxes.hist\u001b[1;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[0;32m   6769\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   6771\u001b[0m \u001b[38;5;66;03m# Massage 'x' for processing.\u001b[39;00m\n\u001b[1;32m-> 6772\u001b[0m x \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39m_reshape_2D(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   6773\u001b[0m nx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)  \u001b[38;5;66;03m# number of datasets\u001b[39;00m\n\u001b[0;32m   6775\u001b[0m \u001b[38;5;66;03m# Process unit information.  _process_unit_info sets the unit and\u001b[39;00m\n\u001b[0;32m   6776\u001b[0m \u001b[38;5;66;03m# converts the first dataset; then we convert each following dataset\u001b[39;00m\n\u001b[0;32m   6777\u001b[0m \u001b[38;5;66;03m# one at a time.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Javie\\Downloads\\Python\\Lib\\site-packages\\matplotlib\\cbook.py:1394\u001b[0m, in \u001b[0;36m_reshape_2D\u001b[1;34m(X, name)\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must have 2 or fewer dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;66;03m# Iterate over list of iterables.\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[]]\n\u001b[0;32m   1397\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO:  First, construct a dictionary of the coefficients to print them\n",
    "coef_dict = None \n",
    "print(f\"Coefficients of the model: {coef_dict}\")\n",
    "\n",
    "# Plot the residuals for the predictions\n",
    "data[\"Predictions\"] = linreg.predict(X)\n",
    "plot_residuals(data, 'Output', 'Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are we seeing here? Is there any way to improve the model?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may expect, you should see some higher-order contribution to the behavior of the data, since some of the residuals have clear structures. In particular, `Input1` has a higher-order polynomial contribution (*which order do you think?*), while `Input2` has an *exponential* form. \n",
    "\n",
    "In order to fit a regression model with these contributions, construct a new dataframe where each column corresponds to the desired manipulation of each variable. Then, fit the regression model and see the results.\n",
    "\n",
    "* Also, pay attention to the fact that you can use whatever contribution you see fit here, not just these previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "data_transf = pd.DataFrame({'Input1': input1 ** None,   # Substitute \"None\" by the degree you think works here\n",
    "                            'Input2': np.exp(input3),\n",
    "                            'Input3': input4,\n",
    "                            'Output': output})\n",
    "X_transf = data_transf.iloc[:,:3].values\n",
    "\n",
    "# Train linear regression model\n",
    "linreg = LinearRegressor()\n",
    "linreg.fit(X_transf, y)\n",
    "\n",
    "# Evaluate the metrics to see the behavior\n",
    "y_pred = linreg.predict(X_transf)\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print again the coefficients for the model and plot the residuals as you did before.\n",
    "\n",
    "**What do you observe?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict = {'Input' + str(i+1): coef for i, coef in enumerate(linreg.coefficients)}\n",
    "print(f\"Coefficients of the model: {coef_dict}\")\n",
    "data_transf[\"Predictions\"] = linreg.predict(X_transf)\n",
    "plot_residuals(data_transf, 'Output', 'Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression, as is, can not deal with categorical variables. Therefore, we need to encode the variables when preprocessing the data. Complete the one-hot-encode function below. Take into account that strings shall be treated automatically as categorical variables.\n",
    "<center>\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize:fit:1358/1*ggtP4a5YaRx6l09KQaYOnw.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Lab_2_4_LR2 import one_hot_encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this in a usual dataset from the internet. If everything is well, you should be able to run the following code as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "\n",
    "y = data['charges'].values\n",
    "X = data.drop(columns=['charges']).values\n",
    "\n",
    "# Preprocess the data\n",
    "# Identify categorical columns for one-hot encoding\n",
    "categorical_columns = [data.columns.get_loc(col) for col in ['sex', 'smoker', 'region']]\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = one_hot_encode(X, categorical_columns, drop_first=True)\n",
    "X_encoded = X_encoded.astype(float)                                  # Watch out for this!\n",
    "\n",
    "# Instantiate and fit the LinearRegressor\n",
    "model = LinearRegressor()\n",
    "model.fit(X_encoded, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_encoded)\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare yourself with scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Preprocess the data\n",
    "# TODO: One-hot encode categorical variables. Use pd.get_dummies()\n",
    "data_encoded = pd.get_dummies(None) \n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data_encoded.drop('charges', axis=1)\n",
    "y = data_encoded['charges']\n",
    "\n",
    "# Instantiate the LinearRegression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colored residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have categorical variables, the relationship between inputs and outputs might differ for different levels of the categorical variables. Therefore, you will modify the `plot_residuals` function to **color the scatter plots based on the value of a specific categorical variable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(data, output_column, prediction_column, categorical_var=None):\n",
    "    \"\"\"\n",
    "    Plot residuals of a model against all variables in the DataFrame. Additionally, plot\n",
    "    a histogram and a QQ-plot of the residuals.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        output_column (str): The name of the output column.\n",
    "        prediction_column (str): The name of the prediction column.\n",
    "        categorical_var (str, optional): The name of a categorical variable for coloring. Defaults to None.\n",
    "    \"\"\"\n",
    "    # TODO: As before, calculate residuals\n",
    "    data['residuals'] = None \n",
    "\n",
    "    # Identify columns to plot (excluding the output and prediction columns)\n",
    "    columns_to_plot = [col for col in data.columns if col not in [output_column, prediction_column, 'residuals']]\n",
    "\n",
    "    # Number of rows and columns for the subplot\n",
    "    n_cols = 3\n",
    "    n_rows = int(len(columns_to_plot) / n_cols) + 2  # Additional row for histogram and QQ-plot\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # TODO: Plot each variable against the residuals\n",
    "    for i, col in enumerate(columns_to_plot):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if categorical_var and categorical_var in data.columns:\n",
    "            sns.scatterplot(x = None, y = None, data=data, ax=ax hue = None)\n",
    "        else:\n",
    "            sns.scatterplot(x = None, y = None, data=data, ax=ax)\n",
    "        \n",
    "        ax.set_title(f'Residuals vs {col}')\n",
    "        ax.axhline(0, ls='--', color='r')\n",
    "\n",
    "    # Histogram of residuals\n",
    "    sns.histplot(data['residuals'], kde=True, ax=axes[i + 1])\n",
    "    axes[i + 1].set_title('Histogram of Residuals')\n",
    "\n",
    "    # QQ-plot of residuals\n",
    "    stats.probplot(data['residuals'], dist=\"norm\", plot=axes[i + 2])\n",
    "    axes[i + 2].set_title('QQ-Plot of Residuals')\n",
    "\n",
    "    # Hide any unused axes\n",
    "    for j in range(i + 3, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "data['predictions'] = model.predict(X)\n",
    "# Example usage\n",
    "plot_residuals(data, 'charges', 'predictions', 'smoker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's happening with the residuals?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Rolling in the deep_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the theory taught in class, do the following:\n",
    "\n",
    "1. **implement the gradient descent algorithm** to train the linear regression model and reproduce the results using this training method. You can find the structure for the gradient descent inside the initial linear regression function, inside the method `fit_gradient_descent`.\n",
    "2. As final steps, **display the progress of the loss function by plotting the gradient descent steps on the X-axis and the loss function on the Y-axis for each step**. \n",
    "3. Additionally, **using the same representation as in the previous section where each axis represents the values of w and b, show the sequence of steps that bring you closer to the optimum each time**. Each step should be a point in space, with coordinates (w,b). Compare all the results with the optimal solution from the scikit fit coefficients.\n",
    "\n",
    "Feel free to add as many cells as you may need from here onwards in order to fulfill these three tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
